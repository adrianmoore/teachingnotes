%\chapter{USSS toolkits}
%shown in figure~\ref{fig:artificialnatural}
%pages~\pageref{tab:opposites1}
%and form (section~\ref{section:form}, page~\pageref{section:form}).

\chapter{Electronic music 3}
\label{history3}


\section{The continued rise of the institution (San Francisco Tape Music Centre, University centres, Rockerfeller and Guggenheim, IRCAM, GRM, UK radiophonic workshop). Morton Subotnik, Chowning, Risset and Matthews (Bell Labs)}

\section{Synthesizers and Computers}
\begin{itemize}
\item History - Hugh le Caine and Oskar Sala
\item Switched on Bach - Wendy Carlos
\item Computers
\item The Institutions - Bell labs.
\item Matthews, Tenney and others
\item The history of the Music N languages
\item CSOUND
\end{itemize}

Two more important (if unrecognized names) in history.

\section{Hugh Le Caine}
Growing up in Ontario, Canada, in his youth Hugh Le Caine displayed equal interest and enthusiasm for music and science. He played a variety of instruments and possessed perfect pitch. He studied Engineering Physics at Queen's University in Kingston, Ontario, also studying music at the Toronto Conservatory of Music in piano. However, with his large and somewhat clumsy hands, he didn't seem to have the touch for delicate playing, and didn't seem to have the potential for a career in performance.
While doing his graduate work, he developed laboratory measurement devices that saved considerable time for researchers and set new standards in physics for measurement precision. Upon graduating with a masters degree in 1939, he began work at the National Research Council in Ottawa in radar technology. He continued work in radar through World War II. After the war, he designed microwave relay systems for transmitting communications. His systems became the standard for satellite relay systems and Doppler weather radar systems that detect moisture in the air.

He also set up a home studio to create new instruments. He gathered communications tools: oscillators, amplifiers, resistors, capacitors, and set out to create an instrument that had flexible control of pitch, volume, and timbre. He wanted to design a keyboard instrument that allowed control of all three.

He called his instrument the Electronic Sackbut. It was a monophonic keyboard with keys that could be moved side to side for vibrato. Pressure on the keys could be varied to control volume. A separate device, operated by the left hand, had a knob and levers to change frequency content. There was also a glide strip that allowed continuous changes in pitch. As for its name, (the Sackbut was an ancestor to the trombone) he said ``the choice of the name of a thoroughly obsolete instrument was thought to afford the designer a certain degree of immunity from criticism.''

In 1948, he went to England to do doctoral studies at the University of Birmingham. There, he heard musique concrete from Paris broadcast on the BBC. When he returned to Canada in 1951, he purchased a two-track tape recorder, a newly available device, and began to experiment with concrete techniques of splicing, changing playback speed, playing tape backwards, etc. He wound up designing the Special-Purpose Tape Recorder. A precursor to the multi-track tape deck, this allowed six stereo 1/4'' tapes to be played simultaneously, with separate speed control on each.

From 1951-1953 he also modified the Electronic Sackbut to create a Touch-Sensitive Organ, which was polyphonic, and played organ-like sounds. The Baldwin Organ Company was interested in it and obtained the rights from the NRC, but never produced touch sensitive keyboards. It appears they simply wanted to keep the rights for themselves so no one else could get an edge on them.

In 1953, Le Caine was asked to demonstrate his instruments to Canada's Scientists' Wives' Association. It was a great success, and subsequently he gave the presentation to the president of the NRC and again for the general public. His humor and musicality are evident on recordings from the time, such as The Sackbut Blues. The president of the NRC was sufficiently impressed to give Le Caine a music studio, and allowed him to pursue his musical instruments full time. At the time, 1954, the only other electronic music studio in North America was at Columbia University, and was tape-based.

At his studio, Le Caine modified his Multi-track to be controlled by a keyboard, so that keys controlled voltage that controlled tape speed, allowing the tapes to be played. He created Dripsody from a single drop of water, Invocation from the same water recording, a ping-pong ball and paddle, a pane of glass being shattered. While not as much in the limelight as Columbia or the European studios, Le Caine showed innovations that were to be emulated for years to come.

\href{http://www.hughlecaine.com}{Hugh Le Caine}, scientist who worked on the development of radar during WWII and afterwards nuclear physics. Best known for the design of an Electronic Sackbut (1945), The National Research Council, Ottawa established a research studio for him. He also designed a multi-track tape recorder (1953). One of his best known works is \textit{Dripsody} (OHM CD1 track 07) (1955) based upon the sound of a drop of water.




Oskar Sala, the German composer and physicist whose novel musical instrument produced the sound effects for Alfred Hitchcock's "The Birds," died Tuesday, February 26 2002. He was 91. Born in the eastern German town of Greiz, Sala is known for developing and mastering the trautonium, billed as the world's first electronic musical instrument on its invention in 1929. He performed with the Berlin Philharmonic several times, and the instrument - a precursor to the synthesizer - was frequently used in German advertss in the 1940s and 1950s.

The trautonium was most famously employed to produce the bird calls in Hitchcock's 1963 film. Few people realized the cacophonous calls on the film were produced electronically. Sala donated his original Mixtur-Trautonium to the German Museum for Contemporary Technology in Bonn in 1995. Example \textit{concertando rubato from elektronische tanzuite} - 1955/1989 (OHM- CD1, track9)

\section{West Coast America}
1963 San Francisco Tape Music Center – arrival of Donald Buchla. The ideas of working with tape (cutting and splicing) were being superseded by synthesizers and the like. Buchla designed something for Morton Subotnick and Ramon Sender. Buchla included an idea for a sequencer. At the request of Subotnick, Buchla made an application to the Rockerfeller foundation for 500 dollars and a 16 step sequencer with touch sensitive keyboard was built. Their work together was to produce the series 100 which eventually Buchla sold the rights to CBS. Subotnick received commissions from Nonesuch Records – for \textit{Silver Apples of the Moon} (1967), \textit{The Wild Bull} (1968) and \textit{Touch} (1969). All completed on the Buchla.

Example (silver apples of the moon). A mechanical landscape: Sequencers, gates, sample and hold circuits. Energy. Part II highlights repeating gestures. Sequencers of 8 or 16 degrees were generated by Buchla. The title of the work comes from a poem by Yeats.

Two other rivals appeared later: Tonus, / ARP in America and EMS Ltd., pinoeered by Peter Zinovieff in England. Voltage control enabled the development of the keyboard to control oscilators. Oscillators could control oscillators. Output of an oscillator is connected across the frequency control break-point of a second … vibrato. The speed is controlled by the frequency of the oscillator, the depth by the amplitude. If the depth of modulation is pronounced and the freq is high (order of 12 hz) it becomes impossible to track the frequency of the wave. (FM)



\subsection{Bell Laboratories and the development of Music N languages}

1948 - Dr. Werner Meyer-Eppler, director of the department of Phonetics at Bonn University was visited by Homer Dudley, researcher at Bell Telephone Labs, New Jersey, USA. Dudley demonstrated the Vocoder (Voice Operated reCOrDER).

Ussachevsky and Luening also discovered research at Bell Telephone laboratories, New Jersey into computer analysis and (re)synthesis of sounds.

\begin{quotation}
The work at Bell Telephone Laboratories proved to be unique. Apart from Ampex, who were involved in the desin and manufacture of recording and amplification equipment, most industrial concerns were not willing even to consider supporting reasearch and development in this field unless they could expect to benefit commercially from the results within a matter of months. \citep[94]{manning2013electronic}
\end{quotation}


Bell Labs, in Murray Hill, New Jersey. Researchers: Max Mathews and John Pierce.

In 1957 Mathews finished Music 1. The first piece produced with MUSIC 1 was a 17second composition by Newman Guttman called The Silver Scale (1957). Mathews - ``it was terrible''. One waveform, a triangle wave. Control over pitch, loudness and duration. Then came numerous revisions of the software – hence the term Music N languages. Music III used the instrument idea and the concept of score and orchestra.

Pierce hired James Tenney who worked in Bell Labs between 1961 and 1964. \textit{Dialogue} (1963) is a study in tone vs. noise.

Music IV was finished – in 1962.

Jean-Claude Risset was doing graduate work in physics at the Ecole Normale Sup\'erieur in Paris but received money to work at Bell in 1964 as research composer in residence. Research: Investigation of instrument tones. (An introductory catalogue of computer synthesized sounds - Risset, 1969)

John Chowning got hold of a copy of the MUSIC IV cards.

The sounds we could make with a computer at that time were unbearably dull, not because we couldn't do better but we didn't know a lot. So in 1967, with ears starved for some sound that had the richness of the sounds we hear in nature, I was experimenting with extreme vibrato. I realized that as I increased the rate and depth of a vibrato, I was no longer hearing it as a change in pitch, but rather as a change in timbre. It was a kind of frequency modulation.
1971 – Chowning hit on the FM theory for good. But few were interested. "the the office of licensing contacted Yamaha, and Yamaha sent an engineer. I played some examples, and in ten minutes he understood." Chowning Patented

Paper published in the Journal of the Audio Engineering Society, September 1973. Research was strong at that time especially with the design and commission of the Samson box, based programmed via a PDP-10 and then functioned as a 'realtime MUSIC IV'. Much money was sifted to this research and Chowning formed CCRMA (Center for Computer Research in Music and Acoustics)…."to have some basis on which we could get more grants." CCRMA became (and still perhaps is) the primary research center. Chowning (Turenas, 1972)

In 1969 JCR moved back to France whereupon he met with Boulez who wanted him to head up the computer department of the newly formed IRCAM. Max Mathews was scientific director at the beginning. Official opening of IRCAM in1977.

Risset - endless glissando (track 40) / music (\textit{mutations}, 1969).

Gerald Bennett worked with Xavier Rodet on CHANT (a vocal synthesis program: this aspect of programming was really important at that time) Bennett and Rodet worked with Johan Sundberg – Swedish acoustician.  First version of CHANT in 1978.

%%%%%%%%%%%%%%%%%%Ballora

\section{Columbia-Princeton Electronic Music Centre}
In 1955, Luening, Ussachevsky and company were creating tape music in a studio housed in Luening's living room. As they began to outgrow the space--and as Luening experienced complaints from his wife from the loss of a living room and complaints from the neighbors about the noise--they went to the president of Columbia University, stating that they required space on campus for their program to continue. They were subsequently moved to a facility on campus. Meanwhile, a new invention was in the works at RCA. Harry F. Olson (1901-1982) was an audio researcher who had started at RCA by developing microphones in the 1930s. In 1948, he had participated in an audio preference test that was designed to determine preferred frequency ranges in audio recordings and broadcasts. (Some researchers had claimed that listeners preferred a top frequency of 5 kHz; RCA found that the reason for this preference was that audio capability at the time had distortion in regions above 5 kHz, so naturally people preferred listening to audio without distortion.) Olson's work had been influenced by Dean Seashore, who did pioneering research in the perception of musical sounds. In the early 1950s, Olson had the idea that it could be possible to create a musical instrument with no limitations. It would be able to produce any sound, and have no mechanical limitations. He stated that acoustic instruments were limited by what humans can do to control them with the mouth, hands, and feet.

Olson began work on the instrument in 1952 with engineer Herbert Belar. It was called the Olson-Belar Sound Synthesizer or the RCA Mark I Sound Synthesizer. They tested it by analyzing piano recordings of Chopin and Debussy and a violin piece by Kreisler and re-creating them with the synthesizer. They analyzed the amplitude, envelope, and spectrum of each note. They found this to be much easier with the piano recordings, since they were composed of discrete note events. Violin performances are characterized by high degrees of vibrato, and slides from one pitch to another, so that all of these parameters are constantly changing. After three years of development and study, they produced a recording of classical music made by the instruments. They mixed excerpts of synthesized and human recordings, and found that listeners had difficulty telling which was which. This assured them and RCA that this new machine was capable of producing worthwhile music.

As the synthesizer was being publicized, Luening and Ussachevsky were given direct access to the president of Columbia for facilities, and preparing a proposal to the Rockefeller Foundation for funding. They prepared a report on the state of experimental music, noting that in Europe electronic music facilities were located in radio stations. They suggested that a more fitting place for a facility in the United States would be a university, where there would be less pressure for commercial releases.

The director of Columbia's School of Arts became interested in the RCA synthesizer, and suggested that Columbia try to obtain it on loan so that they could develop it further. Ussachevsky began to correspond with RCA, discussing the possibility. At the same time, Luening and Ussachevsky contacted Milton Babbitt, a composer at Princeton University who had an interest in electronic music. The three of them formed an idea for a University Council for Electronic Music with equipment distributed among five schools. The Rockefeller Foundation replied that since the three of them already had an alliance, it would be better to have one consolidated facility shared by Columbia and Princeton. Their final proposal requested funding for technical assistants, equipment, space, availability for other composers, plus a concert sound system.


Babbitt, Luening and Ussachevsky. Central to the ``equipment'' request was that the facility would house an RCA synthesizer. Babbitt took an interest in developing the machine further, and had consulted with RCA on design refinements. The cost of the instrument was near 250,000 dollars, far more than the entirety of the Rockefeller grant. Ultimately, RCA agreed to rent the instrument, the RCA Mark II Sound Synthesizer, to the facility. The Columbia-Princeton Electronic Music Center was inaugurated early in 1959 with Babbitt, Luening and Ussachevsky as its directors. The Mark II was constructed with 1700 vacuum tubes. It took up 9 equipment racks -- an entire wall -- and required a large cooling system to dissipate the heat generated by the tubes. It was monophonic -- it took this much hardware to realize one voice, and it all would have had to be duplicated to produce more than one voice.

Information was encoded by punching holes in paper via two typewriter-like keyboards. The paper was drawn over a metal roller, making contact with brushes. Time was expressed in the rate that the holes passed over the brushes. Four-bit binary numbers punched in to the paper could apply to any kind of musical information. Thus, this was a hybrid analog and computer system. The sound generating machinery was analog, while the instructions were entered digitally. The instrument was a prototype when it arrived, and the first order of business was to complete its design and document all of its functionality. This took close to a year. The original construction involved a disc recorder that was synchronized with the paper roll mechanism. Recording to disc was found to be less than ideal, so technician Peter Mauzey replaced the disc mechanism with a tape recorder. The tape had sprockets, so it advanced in synchronization with the paper rollers.

It could be extremely precise with time, frequency, duration, loudness, timbre. It had a feature called the ``frequency glide system'' that allowed pitches to slide from one to another. A pitch could be raised or lowered by 1/6 of a tone. What made it unusual for its time was that it was an integrated system. The electronic instruments in Europe were patched together from radio gear. the synthesizer was constructed with modules that were meant to work together. Sounds could be changed on the fly by simply flipping switches.

\section{Bell Labs}

During World War II, Bell Labs evolved from a communications research firm to a more general think tank. They recruited scientists from many universities to work on radar. To make the offer attractive to all of these innovators, they were given freedom to pursue whatever else interested them at the same time. The facility in Murray Hill, New Jersey, was near a nature preserve. Scientists were encouraged to walk in the woods and think. Artists were also recruited, as it was felt that an aesthetic side was necessary to bring meaning to new technologies, and the scientists were encouraged to pursue any creative interests that they had. Researchers engaged in a variety of pursuits often had adjoining offices, and there was a great deal of creative cross pollenization as a result of neighbors poking their heads into other offices to see what was going on. The open-ended nature of Bell continued through the 1960s, as the race to develop space-based technologies provided continuing impetus for innovation.

\subsection{Max Mathews} (1926-2011) is often called the ``father of computer music.'' After receiving his doctorate in electrical engineering from MIT in 1954, he began work at Bell Labs as the Director of Acoustic and Behavioral Research. His job was to create computer equipment to model and study telephones, to study the audio transmission of telephones and work to improve it. He created some of the first digital audio equipment to digitize sound (analog to digital converter, or ADC), and create sound from digital information (digital to analog converter, or DAC).  This was a revisiting of pulse code modulation (PCM), which Bell Labs had implemented during World War II. PCM represented a ``connect the dots'' method of storing and playing audio. Natural sound waves are continuous, meaning that between any two points on the wave there are an infinite number of possible points in between them. Computers can only operate on discrete values. Thus, in order for a computer to work with audio, the sound pressure wave had to be represented as a series of samples that represented instantaneous amplitude values. The process is analogous to cinematic representation of motion, which is not continuous but rather consists of a series of sampled images that are projected fast enough that motion is perceived by the eye. Using stored samples and a digital to analog converter, the numerical samples may be converted to voltage changes, the voltage changes may be filtered to smooth the transitions from point to point, and the resulting current may drive a loudspeaker and create audio.

Having made this conversion process possible, Mathews became interested in using the computer to create musical audio. The story goes that he and Jonathan Pierce (1910-2002), the executive director of Bell's communication sciences division, attended a piano concert in 1956. Feeling cocky, and perhaps dissatisfied with the performance, one of them remarked to the other, ``The computer can do better than this!'' Mathews then began work creating a new type of software known as an acoustic compiler that could produce musical sounds.

The first such compiler, Music I, was created in 1957. It could generate a single triangle wave, plus control pitch, loudness, and duration. Psychologist Newman Guttman made a short composition called In a Silver Scale, which was an experiment to contrast equal temperament and just intonation. No one raved at the compositional merits of the work, yet, although crude, it was a fascinating beginning. Guttman's second piece, Pitch Variations, used a waveform that consisted of a quick series of pulses. The result was a variation on amplitude modulation, in which the extremely fast volume changes resulted in sum and difference sidebands in addition to the frequency of the wave, resulting in multiple frequencies. These works were in contrast to the work of Lejaren Hiller, who had used the Illiac computer to generate numbers that were then notated and played by humans. In Mathews' implementation, the computer was actually producing the audio. While computers had produced a variety of beeps and buzzes for alerts, this was the first time audio had been produced in a controlled manner to realize musical ideas. Music I was first in a series of seminal software synthesis programs, known collectively as Music N.

The program was written for an IBM 704 computer, which was not located at Bell Labs, but at the IBM World Headquarters in New York City. The samples were written to large computer tape storage reels. A separate machine at Bell Labs could play the sounds back through the digital to analog converter. In 1958 the next version, Music II, was written for the IBM 7094, which was constructed with transistors instead of vacuum tubes. This program was faster, and thus capable of more interesting synthesis algorithms. It was capable of producing four voices, with 16 possible wave shapes (timbres) via the creation of a wavetable oscillator. The wavetable remains the basis of computer music. A wavetable is a description of an arbitrary wave shape, represented as a series of samples. This allowed new types of waveforms to be synthesized, not just the types of waves produced by analog oscillators. The wavetable capitalizes on the fact that pitched sounds are created from periodic waves, waves that repeat regularly. Thus, musical material contains a high degree of redundancy (Claude Shannon might say that music has low entropy). Since music has a high degree of redundancy, it is not necessary to produce samples representing the entire duration of a piece. One description of the wave is all that is needed. A table is made up a number of address locations, each of which contains an amplitude value for the wave. An incrementer reads through the table and sends sample values to a digital to audio converter. That is, the wavetable oscillator has a phase value and a current address in memory. It reads the value stored at the current address, then skips a number of addresses, the number determined by the phase value. When the end of the table is reached, the incrementer "wraps around" to the beginning of a table. Thus, addresses are read in a circular fashion (think of the values on a clock -- it's the same idea). The sampling increment, the number of samples that are skipped from output to output, affects how quickly the table is iterated, and, correspondingly, which pitch is produced. The sampling rate determines how many samples per second are sent to the digital to analog converter.

The stage was set for computers to become a force in the world of music. Mathews and Pierce began searching for musicians to work with. In 1959, Mathews and Guttman gave a talk titled Generation of Music by a Digital Computer at the third International Congress on Acoustics in Stuttgart, Switzerland, where Pitch Variations was played. Among those in the audience was Iannis Xenakis. That same year, they went to the conference of the Audio Engineering Society conference in New York City, where they met Babbitt, Ussachevsky, and Luening, and where Pierce gave a talk titled The Computer as a Musical Instrument. Mathews and Guttman worked at the Columbia Center with Varese to produce a new version of the soundtrack to Deserts. Part of the work involved a visit to Bell Labs, where they made a digital recording of the sound of a buzz saw. Varese thus took an interest in computer music, and included Guttman's second piece, Pitch Variations, to be played at a 1959 concert in New York City featuring his work and the work of John Cage. This was the first public performance of computer music. Bell Labs produced a recording called Music from Mathematics and sent copies to Leonard Bernstein and Aaron Copland. They received a note of thanks from Bernstein's secretary, and a personal reply from Copland, who said, ``The implications are dizzying and, if I were 20, I would be really concerned at the variety of possibilities suggested.''

Music III in 1960 was the culmination of what became foundation principles of computer music. A musical piece realized in two stages, the instruments and the score. The instrument stage incorporated a modular design of unit generators. These are discrete software units that can either output a waveform (e.g., an oscillator unit generator or a noise unit generator) or process a waveform (e.g., a filter unit generator) or do math (e.g., an adding unit generator could combine two signals by adding each of their samples together). Unit generators have one or more inputs, and one output. The output of a unit generator may be directed to the input of another unit generator. Complex instruments could be created by patching together unit generators. A flowchart system of illustration shows the virtual interconnections of unit generators and the mathematical operations that may be done on their outputs. Signal flow starts at the top of the illustration and moves down to the final output. In the illustration, a sawtooth wave oscillator with a frequency of 220 Hz and an amplitude of 0.3 is sent to a lowpass filter. The cutoff frequency of the filter is made to oscillate via a sinewave oscillator that goes through one period every two seconds. The amplitude value of 1800 to the sine oscillator causes it to output values between -1800 and 1800. The adder provides an offset of 2200 to all the amplitude values, thus causing the cutoff frequency value to oscillate between 4000 and 400 Hz.

The score stage was a list of musical events, listed in the order of when they were to occur. Each event was assigned to an instrument, and consisted of a series of values for the unit generators' various parameters (frequency, amplitude, duration, cutoff frequency, etc). Each unit generator and each note event was entered onto a separate punchcard. While punchcards are no longer the way to enter information into a computer, the concept of complex instruments made up of interconnected unit generators remains fundamental to music software and commercial digital instruments.

This incarnation of the software allowed values to change gradually over specified amounts of time. In Mathews' Numerology (1960), the sound of piano gradually changes to that of a bowed string, and an ending during which the rate of note onsets increases to a rate unplayable by any human performer. This ability to gradually change parameters became one of the distinctive features of computer audio, in contrast to the distinctive feature of tape splicing that lent itself to quick juxtapositions of unrelated elements. Composer/historian Kyle Gann observes that many computer composers wrote pieces with a slower unfolding as a result, experimenting with gradually changing parameters.

\section{Stanford}
n 1969, Stanford University in Palo Alto, CA, began holding summer computer workshops, and Stanford quickly blossomed as a leading center for computer music. In 1963, Stanford graduate student John Chowning (1934-) had read Max Mathews' article in Science. He was interested enough to take a course in computer programming and to travel to Bell Labs in the summer of 1964 to meet Mathews. Mathews gave Chowning a copy of Music IV -- which was in the form of a box full of roughly 3,000 punchcards, and a note saying "Good luck!"

Chowning returned to Stanford, where he teamed up with a colleague in computer science. They managed to get Music IV running on one of the computer science's computers, and were able to use a computer in the artificial intelligence laboratory to convert the samples into sound. The two computers shared the same disc space, so that the first computer could generate the samples, store them to disc, and the second computer could read the samples from the same disc and convert them to sound. This was the first integrated computer music system in the world.

In 1966 Chowning received his doctorate in composition at Stanford and joined the faculty. By then, he and his colleagues had re-written Music IV to a version that they called Music 10 (since it was to run on a PDP-10 computer), and a program called SCORE that created note lists for Music 10. Chowning had also created a program with which he program trajectory paths and the computer could create corresponding amplitude changes among four loudspeakers to simulate the motion.

Chowning also made attempts to improve the static nature of the sounds that the computer was producing. In 1967, he began to experiment with vibrato. With a computer, vibrato could both be applied in extreme amounts, and also with precise control. He could be far more systematic than it was possible to be with analog circuits. He noticed that as the vibrato rate and depth increased, the effect transformed from a change in pitch to a change in timbre, and that as he changed the frequencies of both his audio oscillator and his modulating oscillator, it would produce evolving spectra.

He showed his work to an engineer at the AI lab, who told him that he was doing a form of frequency modulation. The differences between what Chowning was doing and what occurred in radio transmission were similar to the differences between Stockhausen's amplitude modulation and radio broadcast. Two oscillators were involved, an audio oscillator (the carrier) and a modulating oscillator that made adjustments to the carrier frequency. Unlike radio broadcast, both oscillators were working at audible frequencies, and there was no demodulation -- instead, Chowning was listening to the modulated wave.

The rich timbres that Chowning perceived were the result of the multiple sidebands that occur in FM transmission -- at sum and difference frequencies of the carrier frequency and integer multiples of the modulator frequency. If the relationship of the carrier to modulator frequency were harmonic, then a series of harmonic sidebands would result. If the relationship were inharmonic, the sidebands would be inharmonic to the carrier. The evolving spectra resulted because the relative amplitudes of FM sidebands is not uniform. They vary according to the amplitude of the modulator, and even slight changes in the modulator amplitude can produce large differences in the amplitudes of the sidebands. The number of sidebands also depends on the amplitude of the modulator. At greater amplitudes, more sidebands appear.

Chowning continued his relationship with Bell Labs, visiting again in 1968, when he met Risset and learned of his work with the analysis of trumpet tones. With other music faculty members, Chowning developed new capabilities of Music 10. In 1969, they gave a summer workshop in computer music, with Max Mathews participating as a visiting professor.

Chowning continued work in simulating the movement of sound objects through space -- along the lines of Stockhausen's multi-channel pieces, but with the precise control of a computer. He expanded his work to include not only motion through amplitude changes, but also by frequency changes to simulate Doppler shifts as objects moved. (The Doppler effect refers to the change in frequency that we hear as objects pass us -- trains or automobiles, as they approach us, sound as though their pitches are getting higher, and when they pass and move away from us, the frequency sounds lower.) The program used an early version of a computer mouse to draw trajectories.

Exploring Risset's ideas, Chowning also found that with FM he could produce effective brass and bell timbres with just two oscillators. By modulating the amplitude of the modulator, he could create dynamic spectral changes along the lines of Risset's research that showed bandwidth to be amplitude-dependent. Stanford's office of technology licensing explored relationships with a number of organ companies, such as Hammond, Wurlitzer and Lowry, but none understood what he was doing enough to show any interest. By chance, a representative from the Japanese company Yamaha was in California. Stanford contacted him, and he arranged for Yamaha to license the technology for a year. In 1973, Chowning, on a sabattical in Europe, wrote a paper on the process that appeared in the Journal of the Audio Engineering Society. This represented a new addition to synthesis methodology, a new approach along with additive and subtractive synthesis. The value was not understood by the Stanford music department who, dismayed over his lack of musical output, dismissed him. Chowning got a grant to work in Germany, although there was not a computer system available such as the one at Stanford. Yamaha, meanwhile, developed the first digital synthesizer, and asked Stanford for a ten-year license to develop FM further. Yamaha obtained the license for the technique in 1974. Stanford, embarrassed, quickly hired Chowning as a research associate.

By this time, a number of researchers at Stanford were working in psychoacoustics, analysis, digital recording, and computer hardware and software. In 1975 they procured grants from the National Science Foundation and the National Endowment for the Arts to create a digital synthesizer. Chowning got support from the music department to form CCRMA (pronounced "karma" -- Center for Computer Research in Music and Acoustics). It was (and is) a multi-disciplinary center to study interactions of engineering, psychology, acoustics, and music. The digital computer arrived in 1977, which enabled them to run Music IV in real time, and produce up to 256 simultaneous, separately controllable sound sources.

\section{IRCAM}
In 1969, Georges Pompidou became president of France. Among his top priorities was a desire that "Paris possess a cultural center that would be both a museum and a center of creation, where visual arts would coexist with music, cinema, books, and audio-visual research."
In 1972 construction began on the Centre Pompidou, which was to be in the heart of Paris. It was to be a structure a million square feet in volume, consisting of four main branches: a museum of modern art, a reference library, a center for industrial design, and a center for music and acoustic research. The plan was also to include office space, book stores, restaurants, cinemas, children's play areas, and parking. The design was by Italian architect Renzo Piano and British architect Richard Rodgers. With its industrial look sharply contrasting the neighboring buildings, it was controversial with its glass facade covered with pipes, and external escalators moving through transparent tubes. People either love it or hate it.

In 1970 Pierre Boulez was made director of the music and acoustic division of the center, IRCAM (Institut de Recherche et Coordination Acoustique/Musique) (Institute for Research and Coordination of Music and Acoustics).
From 1973-78 facilities were constructed in a three-floor subterranean building in the new Centre Pompidou. They included offices, labs, recording studios, an anechoic chamber, and a modular concert hall that could be arranged both in seating and acoustics panels.

IRCAM had five departments:
\begin{itemize}
\item Computer, headed by Jean Risset, which had the resources of Music V and Music 10
\item Instrumental performance
\item Electronic music, headed by Luciano Berio
\item Pedagogy
\item Diagonal, focusing on coordination among the other four parts
\end{itemize}

In 1974 Max Mathews was made scientific advisor. He would spend a few months in Paris each year, and the rest of the time he continued his work at Bell. IRCAM remains a hub of innovation in music technologies. The following are some of the key areas of research that were in progress during the 1970s and early 1980s.

In 1975 Berio brought Guiseppe Di Giugno to Paris from Naples. Di Giugno was a physics teacher at the University of Naples who had become interested in electroacoustic music after hearing Switched on Bach in 1969, and begun creating his own instruments after meeting Bob Moog at a physics conference in 1971. He had formed an electronic music group in 1973. Berio contacted him in 1974, and they began discussing the creation of a digital synthesizer. Berio had joked that the nine oscillators he had in Milan weren't enough, how he'd rather have 1000. Berio arranged to introduce him to Max Mathews, and eventually Di Guigno came to IRCAM to work on a digital synthesizer. He begins the 4 series of digital synthesizers. The 4A in 1976 had increased numbers of oscillators. The 4B in 1977 had interconnections between the oscillators so that they could interact.

In 1977, after completing pieces such as Inharmonique, Risset left IRCAM, finding that there was a great deal of PR, with more of a focus on public appearances, concerts, and less on long term research. In 1978, Risset left to work academia on research projects.

In 1978 the software program CHANT was created by Xavier Rodet, who had been inspired by reading Risset's papers to create a talking computer. He showed a prototype to Gerald Bennett at IRCAM in 1977. The two of them worked with acoustician Johan Sundberg to develop the idea. They researched formant frequencies of vocalists, writing CHANT as a way to model the voice. The program used a simulation of the vocal tract as a synthesis method. It could simulate vocals through analyses carried out every 10 ms. It was extremely computation-intensive.

In 1978 Di Guigno created the 4C, which abstracted the idea of oscillators and interconnection to objects and algorithms that could be linked.

In 1980 Jonathan Harvey composed Mortuos Plango, Vivos Voco, based on recordings of the Winchester Cathedral bell and the voice of a young boy, and made the composition an interplay between the two sounds.

Mathews continued to work on the Conductor program at IRCAM. In 1980 he introduced the Sequential Drum, which was a grid of wires and contact microphones. The signals from the microphones gave information about where and how hard the drum was hit. But it was ultimately not usable for Conductor since it couldn't register continuous controls.

In 1981 Di Guigno finished the 4X synthesizer, which was meant as a universal musical machine for signal processing. It was used by Pierre Boulez for Repons in 1981. Six soloists were processed, and the processed outputs were sent to an array of loudspeakers arranged throughout the hall. This was a breakthrough, the first realtime digital audio processing. Eventually the 4X was marketed, although it was on the order of 100,000 dollars.

In 1981 Rodet introduced FORMES, which worked with the idea of parallel processes (such as volume -- could be a volume process for a sound's onset, or at a larger level, for the volume of a musical passage). It was a step toward control systems to address the many interacting variables that make up music.

